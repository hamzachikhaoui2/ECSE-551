# -*- coding: utf-8 -*-
"""LogisticReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r0aUcxD6BST80_HA9MDZEpBJK8jPG3bY
"""

## ECSE 551 - Mini-Project 1
## Aymen Boustani 260916311, Hamza Chikhaoui 260912960

import numpy as np

# -*- coding: utf-8 -*-
"""LogisticReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r0aUcxD6BST80_HA9MDZEpBJK8jPG3bY
"""

## ECSE 551 - Mini-Project 1
## Aymen Boustani 260916311, Hamza Chikhaoui 260912960

import numpy as np

class LogisticReg:
    # Creates the Logistic Regression Model

    def __init__(self): # Class constructor
        self.w = [] # Initializes an empty weight array

    def __str__(self):
      return "Logistic Regression Model"

    def sigmoid(self, a):
        # Creates the sigmoid function needed later
        return 1 / (1 + np.exp(- a))

    def error(self, w, x, y, lambda_v = 0, reg = None):
      s = sum([(y[i] - self.sigmoid(np.matmul(w.T, x[i]))) * x[i]  for i in range(len(x))])
      if reg == None:
        return s
      elif reg == 'L1': # If we use Lasso regression
        s += lambda_v * np.sign(w)
        return s
      elif reg == 'L2': # If we use Ridge regression
        s += 2 *lambda_v * w
        return s

    # Initializes two learning rate schedulers functions

    def alpha1(self, n, base):
      return base / (1 + 0.01 * n)

    def alpha2(self, a, diff, w):
      return a * diff / np.linalg.norm(w)

    def fit(self, x, y, alpha, epsilon, lambda_v = 0, reg = None, scheduler = None): # Fits our model to the input training data
        # x : training input matrix
        # y : training output column matrix
        assert len(x) == len(y) # Verifies lengths are matching
        w = np.random.rand(len(x[0])) # Initializes a random weight vector
        diff = epsilon + 1
        count = 0
        base = alpha
        while diff >= epsilon: # While the difference between two iterations is still too large
          w_old, delta = w.copy(), 0
          w += alpha * self.error(w, x, y, lambda_v, reg) # Applies the gradient descent update rule
          diff = np.linalg.norm(w - w_old, ord = 4) # Calculates the distance between both vectors
          count += 1
          if scheduler == 'a1':
            alpha = self.alpha1(count, base)
          elif scheduler == 'a2':
            alpha = self.alpha2(alpha, diff, w)
        self.w = w

    def predict(self, x): # Predict class label for a certain sample
      predictions = []
      for x_ in x:
        p = self.sigmoid(np.dot(self.w.T, x_)) # Calculates P(y = 1 | x)
        if p >= 0.5: # Assigns a value depending on the 0.5 treshold
          predictions.append(1)
        else:
          predictions.append(0)
      return predictions # Returns predicted labels for each sample vector

